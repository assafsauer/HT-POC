# SC01-TC01: Enable a vSphere Cluster for Kubernetes Workloads

When vSphere with Kubernetes is enabled on a vSphere cluster, it creates a Kubernetes control plane inside the hypervisor layer. This layer contains specific objects that enable the capability to run Kubernetes workloads within ESXi. A cluster that is enabled for vSphere with Kubernetes is called a Supervisor Cluster (SC). It runs on top of an SDDC layer that consists of ESXi for compute, NSX-T Data Center for networking, and vSAN or another shared storage solution.

---

## Test Case Summary

The test case procedure demonstrates enabling a compatible vSphere 7 cluster as a SC. After enabling the SC, the procedure verifies that the SC Web UI is available for downloading client tools and the API processes authentication, authorization,  and Kubernetes resource requests.

---

## Prerequisites

* vSphere Administrator console and user credentials
* [Licensing for vSphere with Kubernetes](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-kubernetes/GUID-9A190942-BDB1-4A19-BA09-728820A716F2.html) ; vSAN Advanced only required if using vSAN
* [vSphere with Kubernetes System Requirements and Topologies](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-kubernetes/GUID-B1388E77-2EEC-41E2-8681-5AE549D50C77.html)
* [Create Storage Policies for vSphere with Kubernetes](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-kubernetes/GUID-544286A2-A403-4CA5-9C73-8EFF261545E7.html)
* [Configure vSphere with Kubernetes to Use NSX-T Data Center](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-kubernetes/GUID-8D0E905F-9ABB-4CFB-A206-C027F847FAAC.html)

---

## Test Procedure

1. Using the vSphere Administrator console and credentials, login to the VC Web UI.
2. Navigate to **Menu > Workload Management**; from the *Getting Started with Workload Management* drop-down, select the ***vCenter_Server_Name***; select **Enable**.
3. Verify Workload Management recognizes the *vSphere_Cluster_Name* as compatible. If not, revisit [vSphere with Kubernetes System Requirements and Topologies](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-kubernetes/GUID-B1388E77-2EEC-41E2-8681-5AE549D50C77.html) and address any issues preventing workload management compatibility.
4. Select the compatible ***vSphere_Cluster_Name***. Then, select **Next**.
5. For the Control Plane Size option, select **Small**. The, select **Next**.
6. Continue through both *Network* and *Storage* sections, populating all fields for integration with the test environment infrastructure. For additional guidance, reference: [Enable vSphere with Kubernetes on a Cluster with NSX-T Data Center](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-kubernetes/GUID-287138F0-1FFD-4774-BBB9-A1FAB932D1C4.html).
7. Verify that the *Review and Confirm* section reports successful completion of all sections for the planned SC configuration. Then, select **FINISH** to enable the SC.
8. If not automatically redirected, navigate to **Workload Management > Clusters** and periodically monitor the *Config Status* field for configuration updates.
    >Note: During the SC configuration process, disregard any vSphere Recent Tasks errors reporting `HTTP communication could not be complete with status 404` and Config Status errors reporting `Guest operation for the Master node VM with identifier vm-xxxx timed out`. These events represent attempts to contact the control-plane nodes while waiting on the API interface to transition online.

    This step is complete when the *Config Status* value transitions from `Configuring` to `Running`

9. Select the **radio button** to the left of the ***vSphere_Cluster_Name*** then select **EXPORT LOGS** and download the log bundle to a *local directory*.
10. Extract the downloaded log bundle and compressed files for each of the control-node VMs.
11. Explore sample contents in the extracted control-node VMs' directories such as log, command, and verify the manifest files are readable with a text editor.
12. Return to the VC WebUI and navigate to **Workload Management > Clusters**. Record the <i>`Control Plane Node IP Address`</i>. All test cases will reference this value as the <b><i>`SC_API_VIP`</i></b>.
13. Using the **vSphere Administrator** console and credentials, login to the SC API. Reference the [Configuration Supplement](../supplements/client-configuration.md##Login-to-a-Supervisor-Cluster-as-a-vCenter-Single-Sign-On-User) for details on obtaining the CLI tools and command syntax.

14. Set the context for the SC with the command.
    <pre>kubectl config use-context <i><b>SC_API_VIP</b></i></pre>
15. Verify status of SC, Kubernetes master and URL with the command 

    ```sh
    kubectl cluster-info
    ```

16. Verify all SC control-plane nodes are available API server endpoints.

    ```sh
    kubectl get endpoints
    ```

17. Verify status, role, and version for all control-plane nodes and worker nodes.

    ```sh
    kubectl get nodes
    ```

18. Run to verify all SC pods report either `Running` or `Completed`.

    ```sh
    kubectl get pods -A
    ```

---

## Test Data

None

---

## Expected Results

Step | Result |
--- | --- |
7  | You have successfully completed all the steps required to enable namespaces on <i>vSphere_Cluster_Name</i> … |
8 | *Config Status* is: **Running** |
11 | Log, command, and manifest files are readable |
13 | <pre>Logged in successfully.<br>You have access to the following contexts:<br><b><i>SC_API_VIP</i></b></pre> |
15 | <pre>Kubernetes master is running at https://<b><i>SC_API_VIP</i>:6443<br>...</pre> |
16 | <pre>NAME         ENDPOINTS<br>kubernetes   <i>Ctrl_Plane_Node_01_IP</i>:6443,<i>Ctrl_Plane_Node_02_IP</i>:6443,<i>Ctrl_Plane_Node_03_IP</i>:6443</pre> |
17 | <pre>NAME                        STATUS   ROLES   VERSION<br><i>Ctrl_Plane_Node_01_UUID</i>     Ready    master  v1.16.7~<br><i>Ctrl_Plane_Node_03_UUID</i>     Ready    master  v1.16.7~<br><i>Ctrl_Plane_Node_02_UUID</i>     Ready    master  v1.16.7~<br><i>ESXi_Host_01_FQDN/IP</i>        Ready    agent   v1.16.7~<br><i>ESXi_Host_02_FQDN/IP</i>        Ready    agent   v1.16.7~<br><i>ESXi_Host_03_FQDN/IP</i>        Ready    agent   v1.16.7~<br>…</pre>Note: VMware does not update this document with every TKG release. The versions reported above represent sample output.|
18 | All pods report `STATUS` as either **`Running`** or **`Completed`** |

---

## Actual Results

Step | Result |
--- | --- |
7  | |
8 | |
11 | |
13 | |
15 | |
16 | |
17 | |
18 | |

---

## Status Pass/Fail

* [  ] Pass
* [  ] Fail

Return to [Test Cases Inventory](../README.md#test-cases-inventory)
